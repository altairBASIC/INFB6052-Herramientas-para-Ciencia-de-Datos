{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "p_chA4mio1iH",
      "metadata": {
        "id": "p_chA4mio1iH"
      },
      "source": [
        "# Tarea: Mapa paralelo — Comparación de herramientas (Comercial vs Open Source)\n",
        "**Curso: HERRAMIENTAS PARA CIENCIA DE DATOS**  \n",
        "**Integrantes: Ignacio Ramírez, Antonia Montecinos, Cristian Vergara**  \n",
        "**Profesor(a): MICHAEL GABRIEL MIRANDA SANDOVAL**  \n",
        "**Fecha de entrega: 05-11-2025**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uP0Ux-Tv5EXl",
      "metadata": {
        "id": "uP0Ux-Tv5EXl"
      },
      "source": [
        "## 1. Introducción y Objetivo \n",
        "\n",
        "En esta investigación comparamos **AWS Glue** (comercial, servicio gestionado en AWS) y **Apache Airflow** (open source) dentro del ítem **Orquestación / ETL–ELT (DTS)**. El foco es **documental** y se basa en **ejemplos oficiales de terceros** (tutoriales, guías y blogs técnicos) correctamente referenciados. No implementamos código propio nuevo; describimos y analizamos flujos ya publicados y sus implicancias prácticas.\n",
        "\n",
        "**Contexto y motivación.** Muchas organizaciones deben elegir entre un **motor ETL gestionado** (p. ej., Glue/Spark) y un **orquestador agnóstico al motor** (Airflow). Elegir incorrectamente introduce fricción (coste, gobernanza, mantenimiento y time-to-value). Esta investigación busca aportar criterios concretos para decidir.\n",
        "\n",
        "**Objetivo general.** Evaluar ventajas, desventajas y casos de uso recomendados de AWS Glue y Apache Airflow para pipelines de datos de ingesta, transformación y carga.\n",
        "\n",
        "**Objetivos específicos.**\n",
        "1. **Caracterizar** el rol de cada herramienta (motor ETL vs orquestador) y su ecosistema.\n",
        "2. **Describir** uno o más ejemplos canónicos de uso.\n",
        "3. **Comparar** en criterios técnicos y operativos: prototipado, escalabilidad, coste, integraciones, reproducibilidad y gobernanza.\n",
        "4. **Recomendar** cuándo usar cada opción (empresa pequeña vs grande; mono-nube vs multi-nube).\n",
        "\n",
        "\n",
        "**Metodología.**\n",
        "- Selección de **fuentes primarias** (docs oficiales) y **secundarias** (blogs técnicos de AWS/Apache).\n",
        "- Extracción de: arquitectura, pasos, dependencias, outputs, requisitos y notas de operación.\n",
        "- Síntesis comparativa con **tabla** y **preguntas guía**.\n",
        "- Revisión por pares (equipo) para sesgos y coherencia.\n",
        "\n",
        "**Preguntas guía:**\n",
        "- ¿Cuál sirve mejor para prototipado rápido? ¿y para producción?  \n",
        "- ¿Implicancias de coste/gobernanza?  \n",
        "- ¿Limitaciones vistas en documentación/ejemplos?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b49df296",
      "metadata": {
        "id": "b49df296"
      },
      "source": [
        "## Resumen ejecutivo (150–200 palabras) \n",
        "\n",
        "Este trabajo compara AWS Glue (comercial) y Apache Airflow (open source) para orquestación y ejecución de flujos ETL/ELT. Se basa exclusivamente en ejemplos de terceros: tutoriales oficiales de AWS y de Apache Airflow que muestran pipelines básicos de ingesta, transformación y carga. Metodológicamente, documentamos los pasos, dependencias y resultados descritos por esas fuentes, sin implementar código nuevo.\n",
        "### hallazgos principales\n",
        "(1) AWS Glue es un servicio ETL serverless gestionado, óptimo para transformaciones pesadas con Apache Spark dentro del ecosistema AWS; expone un editor visual (Glue Studio) que acelera el prototipado y genera código PySpark editable.\n",
        "\n",
        "\n",
        "(2) Apache Airflow es un orquestador puro y agnóstico al motor, con gran flexibilidad en Python y ecosistema de providers; requiere administrar infraestructura (Docker/Kubernetes/VM) y una base de metadatos.\n",
        "\n",
        " En costes, Glue cobra por uso (DPU-hora) y reduce la operación de servidores, mientras que Airflow es software libre pero demanda recursos de operación 24/7 para scheduler/webserver/workers.\n",
        "\n",
        "### Recomendacion\n",
        " **Glue** es preferible para ETL de **alto volumen centrado en AWS** y equipos que priorizan velocidad operativa con menor gestión. **Airflow** es preferible como **orquestador central** en entornos **multi-nube** o con diversidad de motores; suele **complementar** a Glue en producción orquestando jobs propios de Glue/DBT/SQL. Para una pyme 100% AWS: Glue + Step Functions/Airflow ligero; para una empresa con múltiples plataformas: Airflow como plano de orquestación y motores especializados por tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZSLFXAlO95-m",
      "metadata": {
        "id": "ZSLFXAlO95-m"
      },
      "source": [
        "## 2. Selección de Herramientas \n",
        "\n",
        "**Ítem:**  \n",
        "Orquestación / ETL–ELT (DTS)\n",
        "\n",
        "**Comercial :**  \n",
        "AWS Glue (servicio gestionado en AWS; documentación oficial, versión 2025)\n",
        "\n",
        "**Open Source:**  \n",
        "Apache Airflow v3.1.2 (versión estable; documentación consultada 3-Nov-2025)\n",
        "\n",
        "**Justificación breve :**  \n",
        "Seleccionamos AWS Glue y Apache Airflow por representar dos enfoques complementarios de orquestación de datos.  \n",
        "AWS Glue ofrece un modelo ETL totalmente gestionado y serverless con motor Spark y editor visual (Glue Studio), ideal para pipelines sobre infraestructura AWS con bajo mantenimiento.  \n",
        "Apache Airflow, en cambio, es un orquestador open source y agnóstico al motor, orientado a flujos heterogéneos (SQL, Spark, APIs) y ampliamente adoptado en entornos multi-nube, aunque requiere administración de infraestructura (scheduler/webserver/workers).\n",
        "\n",
        "**Ejemplo(s) de terceros usados (enlace — autor/organización — fecha — qué parte se usa):**\n",
        "\n",
        "1. **AWS Glue — Tutorial oficial (Crawler + Catálogo)**  \n",
        "   - Enlace: https://docs.aws.amazon.com/glue/latest/dg/tutorial-add-crawler.html\n",
        "   - Autor/Organización: AWS Docs  \n",
        "   - Fecha: Accedido 3-Nov-2025 (© 2025)  \n",
        "   - Parte usada: Ejemplo de creación de Crawler para inferir esquema desde S3 y registrar tablas en Glue Data Catalog.\n",
        "\n",
        "2. **AWS Glue Studio — Blog de AWS Big Data (pipeline extremo a extremo)**  \n",
        "   - Enlace:https://aws.amazon.com/blogs/big-data/end-to-end-development-lifecycle-for-data-engineers-to-build-a-data-integration-pipeline-using-aws-glue\n",
        "   - Autor/Organización: AWS Big Data Blog (Noritaka Sekiyama)  \n",
        "   - Fecha: 26-Jul-2023  \n",
        "   - Parte usada: Flujo de diseño e implementación de jobs ETL con Glue Studio, integración con S3, Redshift y CDK.\n",
        "\n",
        "3. **AWS Glue Studio - Making ETL easier with AWS Glue Studio**\n",
        "   - Enlace: [https://aws.amazon.com/es/blogs/big-data/making-etl-easier-with-aws-glue-studio/](https://aws.amazon.com/es/blogs/big-data/making-etl-easier-with-aws-glue-studio/)  \n",
        "   - Autor/Organización: AWS Big Data Blog (Autor: Leonardo Gomez)  \n",
        "   - Fecha: 24-Sep-2020  \n",
        "   - Parte usada Creacion de  un trabajo de ETL con AWS Glue Studio.\n",
        "\n",
        "4. **Apache Airflow — Tutorial oficial (Pipeline simple con Postgres)**  \n",
        "   - Enlace: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/pipeline.html\n",
        "   - Autor/Organización: Apache Airflow Docs  \n",
        "   - Fecha: Documentación v3.1.2 (consultado 3-Nov-2025)  \n",
        "   - Parte usada: Ejemplo DAG que descarga CSV, carga en Postgres y aplica transformaciones SQL; base para comparación con Glue.\n",
        "\n",
        "5. **Apache Airflow — TaskFlow API (ETL Python-first)**  \n",
        "   - Enlace: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html\n",
        "   - Autor/Organización: Apache Airflow Docs  \n",
        "   - Fecha: Documentación v3.1.2 (consultado 3-Nov-2025)  \n",
        "   - Parte usada: Ejemplo ETL modular con decoradores @dag y @task; muestra flujos nativos Python y visualización DAG.\n",
        "\n",
        "6. **Apache Airflow — UI Overview**  \n",
        "   - Enlace: https://airflow.apache.org/docs/apache-airflow/3.1.2/ui.html\n",
        "   - Autor/Organización: Apache Airflow Docs  \n",
        "   - Fecha: Documentación v3.1.2 (consultado 3-Nov-2025)  \n",
        "   - Parte usada: Vista de la Interfaz de usuario de Airflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10d3cde6",
      "metadata": {
        "id": "10d3cde6"
      },
      "source": [
        "# 3. Configuración del Entorno\n",
        "\n",
        "- **Entorno Comercial (AWS Glue):**\n",
        "  - Requiere una cuenta activa de AWS.\n",
        "  - Recursos mínimos: bucket(s) S3 (origen/destino), rol IAM para Glue (por ejemplo, `AWSGlueServiceRole-*`), y opcionalmente destino analítico (Amazon Redshift/RDS/Athena).\n",
        "  - Toda la configuración es vía consola AWS (Glue Studio, S3, IAM). El job corre en Spark serverless (DPUs administradas por AWS).\n",
        "\n",
        "- **Entorno Open Source (Apache Airflow):**\n",
        "  - Requiere una instancia de Airflow; el tutorial oficial usa Docker + Docker Compose.\n",
        "  - Componentes: scheduler, webserver, base de metadatos, y workers (según deployment).\n",
        "  - Paquetes/Providers de ejemplo: `apache-airflow`, `apache-airflow-providers-postgres`, `requests`.\n",
        "  - Recursos: contenedor Postgres (para el tutorial) y conexiones definidas en la UI (por ej., `tutorial_pg_conn`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WXgLcB-Wjb62",
      "metadata": {
        "id": "WXgLcB-Wjb62"
      },
      "source": [
        "# 4. Ejemplo de Uso: Herramienta Comercial (AWS Glue) (mooodificado)\n",
        "\n",
        "En esta sección se documenta un ejemplo de uso de la herramienta **comercial seleccionada (AWS Glue)**.  \n",
        "No se implementó el ejemplo directamente, sino que se basa en **documentación y tutoriales oficiales** de AWS que muestran el flujo completo de un pipeline ETL.  \n",
        "\n",
        "---\n",
        "\n",
        "###  Fuente principal\n",
        "- **Título:** *Adding an AWS Glue Crawler*  \n",
        "- **Enlace:** [https://docs.aws.amazon.com/glue/latest/dg/tutorial-add-crawler.html](https://docs.aws.amazon.com/glue/latest/dg/tutorial-add-crawler.html)  \n",
        "- **Autor/Organización:** AWS Docs  \n",
        "- **Fecha:** Accedido 3-Nov-2025 (© 2025)  \n",
        "\n",
        "**Qué hace el ejemplo:**  \n",
        "Crea y ejecuta un *Crawler* que analiza archivos CSV almacenados en un bucket S3, infiere automáticamente su esquema y los registra en el **Glue Data Catalog**, dejando las tablas disponibles para futuros trabajos ETL.  \n",
        "\n",
        "**Resultados generados:**  \n",
        "- Tablas catalogadas con columnas, tipos de datos y particiones inferidas.  \n",
        "- Registro automático en el catálogo, visible en la consola AWS Glue Studio.  \n",
        "- Punto de partida para ejecutar jobs ETL visuales o basados en PySpark.  \n",
        "\n",
        "---\n",
        "\n",
        "![Creacion de un Crawler](fotos/fuente1..png)\n",
        "\n",
        "###  Fuente complementaria\n",
        "- **Título:** *End-to-End Development Lifecycle for Data Engineers using AWS Glue*  \n",
        "- **Enlace:** [https://aws.amazon.com/blogs/big-data/end-to-end-development-lifecycle-for-data-engineers-to-build-a-data-integration-pipeline-using-aws-glue](https://aws.amazon.com/blogs/big-data/end-to-end-development-lifecycle-for-data-engineers-to-build-a-data-integration-pipeline-using-aws-glue)  \n",
        "- **Autor/Organización:** AWS Big Data Blog (Autor: Noritaka Sekiyama)  \n",
        "- **Fecha:** 26-Jul-2023  \n",
        "\n",
        "**Qué hace el ejemplo:**  \n",
        "Muestra un pipeline de integración de datos completo desarrollado con **AWS Glue Studio** y **AWS CDK**, abarcando las etapas *Plan → Design → Implement → Test → Deploy → Maintain*.  \n",
        "Incluye la creación de buckets S3, definición de roles IAM, desarrollo de scripts PySpark, testing con Docker y despliegue automatizado mediante CodePipeline y CodeCommit.  \n",
        "\n",
        "**Resultados generados:**  \n",
        "- Job ETL que lee datos JSON desde S3, aplica transformaciones con Spark y los escribe en Redshift/S3.  \n",
        "- Grafo visual del flujo en Glue Studio (fuente → transform → destino).  \n",
        "- Métricas de ejecución y uso de DPU (Spark serverless).  \n",
        "\n",
        "\n",
        "![diagrama pipeline](fotos/fuente22.png)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###  Fuente complementaria\n",
        "- **Título:** *Making ETL easier with AWS Glue Studio*  \n",
        "- **Enlace:** [https://aws.amazon.com/es/blogs/big-data/making-etl-easier-with-aws-glue-studio/](https://aws.amazon.com/es/blogs/big-data/making-etl-easier-with-aws-glue-studio/)  \n",
        "- **Autor/Organización:** AWS Big Data Blog (Autor: Leonardo Gomez)  \n",
        "- **Fecha:** 24-Sep-2020  \n",
        "\n",
        "**Qué hace el ejemplo:**  \n",
        "\n",
        "Muestra cómo AWS Glue Studio permite crear y orquestar procesos ETL de manera visual, combinando múltiples fuentes S3,\n",
        " aplicando mapeos y uniones, agregando métricas y finalmente generando una tabla consolidada sin necesidad de escribir Spark desde cero\n",
        "\n",
        "**Resultados generados:**  \n",
        "- Un dataset limpio, unificado y agregado, con el conteo de tickets agrupado según criterios definidos (por ejemplo, por tipo de prueba o periodo).\n",
        "- Datos almacenados en S3 listos para análisis, visualización o carga a un Data Warehouse.\n",
        "- Ejemplo visual de pipeline ETL completo construido en Glue Studio:\n",
        "desde la extracción, pasando por la transformación, hasta la carga del resultado final.\n",
        "\n",
        "\n",
        "![ETL TERMINADO](fotos/fuenten.jpg)\n",
        "\n",
        "---\n",
        "\n",
        "###  Reproducibilidad del ejemplo\n",
        "Los ejemplos de AWS Glue **son ejecutables por terceros** mediante una cuenta activa en AWS.  \n",
        "Para reproducirlos, basta con:\n",
        "1. Crear un bucket S3 con datos de muestra (por ejemplo, archivos CSV públicos).  \n",
        "2. Crear un *Crawler* siguiendo el tutorial oficial y registrar el esquema en Glue Data Catalog.  \n",
        "3. Diseñar un job ETL en Glue Studio con origen S3 → transformación Spark → destino S3/Redshift.  \n",
        "\n",
        "No se ejecutó en este trabajo porque AWS Glue requiere **recursos facturables (DPU-hora)** y **roles IAM activos**; sin embargo, los pasos de reproducción están completamente documentados en las fuentes oficiales citadas.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###  Observaciones\n",
        "- **Facilidad de uso:** Alta; el editor visual de Glue Studio acelera la creación de flujos sin requerir configuración de clústeres Spark.  \n",
        "- **Coste:** Basado en consumo (*pago por DPU-hora*); sin costos fijos de infraestructura.  \n",
        "- **Limitaciones:** Dependencia total del ecosistema AWS (lock-in).  \n",
        "- **Ventajas:** Orquestación simple, entorno serverless, y despliegue gestionado.  \n",
        "- **Créditos:** Ejemplo documentado oficialmente por AWS Docs y AWS Big Data Blog.\n",
        " Parte 5 – Ejemplo de Uso: Herramienta Open Source (Apache Airflow)\n",
        "(con el mismo formato oficial que tu guía)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aTePAvaxoFam",
      "metadata": {
        "id": "aTePAvaxoFam"
      },
      "source": [
        "## 5. Ejemplo de Uso: Herramienta Open Source (Apache Airflow)  (modificado)\n",
        "\n",
        "En esta sección se documenta un ejemplo de uso de la herramienta **open source seleccionada (Apache Airflow)**.  \n",
        "El contenido proviene de ejemplos oficiales publicados en la documentación de Apache y muestra el flujo completo de un pipeline ETL orquestado en Airflow.\n",
        "\n",
        "---\n",
        "\n",
        "###  Fuente principal\n",
        "- **Título:** *Building a Simple Data Pipeline*  \n",
        "- **Enlace:** [https://airflow.apache.org/docs/apache-airflow/stable/tutorial/pipeline.html](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/pipeline.html)  \n",
        "- **Autor/Organización:** Apache Airflow Docs  \n",
        "- **Fecha:** Consultado 3-Nov-2025 (versión 3.1.2)  \n",
        "\n",
        "**Qué hace el ejemplo:**  \n",
        "Define un DAG que (1) descarga un archivo CSV, (2) lo carga a una tabla staging en PostgreSQL y (3) ejecuta tareas de limpieza y combinación de datos usando operadores SQL (`SQLExecuteQueryOperator`, `PostgresHook`).  \n",
        "Incluye el uso de Docker Compose para inicializar el entorno (scheduler, webserver, database).  \n",
        "\n",
        "**Resultados generados:**  \n",
        "- DAG ejecutable y visualizable en la interfaz web (List, Graph y Grid views).  \n",
        "- Logs detallados por tarea y estado de ejecución (*success, failed, retry*).  \n",
        "- Pipeline reproducible localmente vía Docker con base de datos Postgres.  \n",
        "\n",
        "![aa](fotos/fuente3.png)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###  Fuente complementaria\n",
        "- **Título:** *Pythonic DAGs with the TaskFlow API*  \n",
        "- **Enlace:** [https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)  \n",
        "- **Autor/Organización:** Apache Airflow Docs  \n",
        "- **Fecha:** Consultado 3-Nov-2025 (versión 3.1.2)  \n",
        "\n",
        "**Qué hace el ejemplo:**  \n",
        "Implementa un flujo ETL usando la API *TaskFlow*, que simplifica la definición de dependencias mediante decoradores `@dag` y `@task`.  \n",
        "Muestra cómo pasar datos entre tareas y visualizar dependencias en la UI de Airflow.\n",
        "\n",
        "**Resultados generados:**  \n",
        "- DAG modular con dependencias tipo `t1 >> t2 >> t3`.  \n",
        "- Captura de vista Graph mostrando tareas enlazadas.  \n",
        "- Ejecución exitosa con celdas en verde y logs por tarea.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###  Fuente complementaria\n",
        "- **Título:** *UI Overview*  \n",
        "- **Enlace:** [https://airflow.apache.org/docs/apache-airflow/3.1.2/ui.html](https://airflow.apache.org/docs/apache-airflow/3.1.2/ui.html)  \n",
        "- **Autor/Organización:** Apache Airflow Docs  \n",
        "- **Fecha:** Consultado 3-Nov-2025 (versión 3.1.2)  \n",
        "\n",
        "**Qué hace el ejemplo:**  \n",
        "Presenta la descripción de la interfaz de usuario (UI) de Airflow 3.1.2 que permite visualizar, gestionar y depurar los flujos de trabajo (DAGs) del sistema. La UI cubre vistas clave como Home/Dashboard, lista de DAGs, detalles de DAG con sus vistas Grid, Graph, Code, etc.; habilita también la visualización del estado del sistema y operaciones de mantenimiento\n",
        "\n",
        "**Resultados generados:**  \n",
        "- Vista Home/Dashboard que muestra la salud del sistema, números de tareas en ejecución o fallidas, y accesos rápidos a DAGs críticos.  \n",
        "- Vista DAG List que lista todos los DAGs, con su estado, próxima ejecución, etiquetas y botones de pausa/ejecución.  \n",
        "- Acciones operativas desde la UI: pausar/reanudar DAGs, ejecutar DAGs manualmente, backfills, ver logs por tarea, marcar tareas exitosas/limpiar.\n",
        "\n",
        "![grapho terminado](fotos/dag_run_graph1.png)\n",
        "\n",
        "---\n",
        "\n",
        "###  Reproducibilidad del ejemplo\n",
        "Los ejemplos de Apache Airflow **pueden reproducirse localmente** mediante Docker Compose, utilizando los archivos disponibles en la documentación oficial.  \n",
        "Pasos generales:\n",
        "1. Instalar Docker y Docker Compose.  \n",
        "2. Clonar o crear la estructura del proyecto con el archivo `docker-compose.yaml` provisto por Apache.  \n",
        "3. Ejecutar los comandos:\n",
        "   ```bash\n",
        "   docker compose up airflow-init\n",
        "   docker compose u\n",
        "\n",
        "---\n",
        "\n",
        "###  Observaciones\n",
        "- **Facilidad de uso:** Moderada; requiere instalación o despliegue con Docker, pero otorga total flexibilidad sobre los flujos.  \n",
        "- **Extensibilidad:** Muy alta; se integra con *providers* oficiales para AWS, GCP, Azure, Databricks, DBT y APIs externas.  \n",
        "- **Mantenimiento:** Requiere operación del scheduler, base de metadatos y workers (mayor carga operativa que Glue).  \n",
        "- **Ventajas:** Orquestador agnóstico, adaptable y estándar en la industria OSS.  \n",
        "- **Comunidad:** Amplia, activa y con soporte continuo de Apache Foundation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- **Evidencia (código y capturas, según docs):**\n",
        "  - Fragmento conceptual: Definición de DAG (TaskFlow/Operators) y dependencias tipo `t1 >> t2 >> t3` (o invocación funcional TaskFlow).\n",
        "  - Captura 1: Vista Graph con las 3 tareas y sus dependencias.\n",
        "  - Captura 2: Vista Grid mostrando una corrida exitosa (celdas en verde) y acceso a logs por tarea.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uB0_KOst0xfI",
      "metadata": {
        "id": "uB0_KOst0xfI"
      },
      "source": [
        "# 6. Comparación de la Herramientas (modificado)\n",
        "\n",
        "En esta sección se comparan de forma estructurada los resultados  con ambas herramientas: **AWS Glue (comercial)** y **Apache Airflow (open source)**.  \n",
        "Dado que los ejemplos utilizados provienen de documentación oficial, la comparación se centra en **criterios técnicos y prácticos** observados en la documentación, en la facilidad de uso y en el enfoque arquitectónico de cada solución.\n",
        "\n",
        "| Criterio | Comercial (AWS Glue) | Open Source (Apache Airflow) | Observaciones | Recomendación |\n",
        "|---|---|---|---|---|\n",
        "| **Rol principal** | Motor ETL serverless (Spark) + orquestación simple | Orquestador puro (agnóstico al motor) | Glue ejecuta el Spark; Airflow coordina y puede disparar jobs de Glue | Se complementan |\n",
        "| **Facilidad de prototipado** | Alta (Glue Studio visual) | Alta (código + TaskFlow) | Glue acelera el desarrollo sin configurar clústeres; Airflow requiere definir tareas manualmente. | Glue para ETL rápido en AWS; Airflow para flujos personalizados. |\n",
        "| **Escalabilidad y rendimiento** | Automática (DPU administradas por AWS, escalado transparente) | Depende de workers/cluster |Glue escala sin intervención; Airflow escala horizontalmente en Kubernetes o Celery | Glue en workloads controlados, Airflow en entornos distribuidos |\n",
        "| **Coste y modelo de uso** | Pago por DPU-hora | Gratuito (licencia OSS) pero requiere infraestructura y mantenimiento | Glue implica gasto operativo (OpEx); Airflow implica coste de operación local (infra/tiempo) |Airflow si ya se dispone de infraestructura; Glue si se busca simplicidad |\n",
        "| **Integraciones** | Nativas AWS (S3, Redshift, RDS, IAM) | Provedores (AWS/GCP/Azure/DBs/APIs) | Glue depende del ecosistema AWS; Airflow es agnóstico | Airflow para multi-nube; Glue para entornos 100% AWS |\n",
        "| **Documentación y soporte** | Muy detallada (AWS Docs, blogs oficiales, ejemplos GUI) | Extensa y comunitaria (Apache Docs, foros, GitHub) | Glue ofrece guías visuales; Airflow tiene documentación extensa y técnica | Glue más visual, Airflow más técnico |\n",
        "| **Facilidad de acceso y reproducción** | Requiere cuenta AWS y permisos IAM | Requiere entorno Docker local (documentación oficial) |Ambos reproducibles; Glue en nube, Airflow en local | Depende del entorno del usuario (cloud vs on-premise) |\n",
        "| **Curva de aprendizaje** | Media (Spark, IAM, AWS) | Media-Alta (DAGs, Docker/K8s) | Perfila al equipo: data engineers vs. platform/devops | Depende del equipo |\n",
        "\n",
        "### Preguntas guía\n",
        "- **Prototipado rápido:** Glue Studio es muy rápido para ETL de Spark en AWS; Airflow es ágil para definir flujos heterogéneos de tareas.\n",
        "- **Producción:** Usualmente se combinan. Airflow orquesta y llama jobs de Glue dentro de un DAG.\n",
        "- **Coste/Gobernanza:** Glue es OpEx por uso; Airflow implica coste de infraestructura y operación. Glue integra IAM; Airflow gestiona usuarios/roles en su propio stack.\n",
        "- **Limitaciones:** Glue: vendor lock-in y orquestación simple. Airflow: no procesa datos por sí mismo y requiere mantenimiento."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ROAm_s5zey",
      "metadata": {
        "id": "53ROAm_s5zey"
      },
      "source": [
        "## 7. Conclusiones y Reflexión (modificado)\n",
        "\n",
        "El análisis comparativo entre **AWS Glue** (herramienta comercial) y **Apache Airflow** (herramienta open source) permite extraer conclusiones claras sobre sus **enfoques, fortalezas y limitaciones** en el ámbito de la orquestación y ejecución de flujos ETL/ELT.\n",
        "\n",
        "---\n",
        "\n",
        "###  Conclusiones técnicas\n",
        "\n",
        "**AWS Glue**  \n",
        "- Representa un enfoque *ETL gestionado y serverless*, óptimo para entornos completamente integrados en AWS.  \n",
        "- Su editor visual (Glue Studio) facilita el **prototipado rápido** sin requerir configuración de infraestructura ni clústeres Spark.  \n",
        "- Ofrece escalabilidad automática y seguridad centralizada mediante IAM.  \n",
        "- Principal limitación: **dependencia del ecosistema AWS (vendor lock-in)**, con escasa flexibilidad fuera de su entorno.\n",
        "\n",
        "**Apache Airflow**  \n",
        "- Es un **orquestador agnóstico** y programable, que se adapta a cualquier motor o servicio (Spark, SQL, DBT, APIs, etc.).  \n",
        "- Permite definir flujos complejos y reproducibles mediante código Python, favoreciendo la **extensibilidad y mantenibilidad**.  \n",
        "- Su arquitectura requiere despliegue y mantenimiento (scheduler, webserver, metadatabase), lo que implica **mayor carga operativa**.  \n",
        "- Ideal para flujos **multi-nube o híbridos**, donde se coordinen diversos servicios y fuentes de datos.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Ambas herramientas **no compiten directamente**, sino que **se complementan**:  \n",
        "- Glue cumple el rol de **motor ETL serverless**, ejecutando transformaciones en Spark.  \n",
        "- Airflow actúa como **director/orquestador**, gestionando dependencias entre tareas y flujos heterogéneos.  \n",
        "\n",
        "En la práctica, las empresas combinan ambos enfoques: Airflow puede disparar jobs de Glue o Spark, integrándolos en pipelines unificados.  \n",
        "La elección depende del **balance entre simplicidad, autonomía técnica y contexto de infraestructura**.\n",
        "\n",
        "---\n",
        "\n",
        "###  Recomendaciones según contexto\n",
        "\n",
        "| Tipo de organización | Recomendación |\n",
        "|----------------------|----------------|\n",
        "| **Pequeña / 100% AWS** | Adoptar **AWS Glue** como solución principal de ETL y orquestación básica, aprovechando su entorno serverless y escalabilidad automática. |\n",
        "| **Mediana o grande / multi-nube** | Utilizar **Apache Airflow** como orquestador central, integrando Glue, DBT, Databricks o SQL según cada caso. |\n",
        "| **Equipos mixtos (data + devops)** | Combinar Glue para cargas ETL y Airflow para coordinar tareas y dependencias complejas. |\n",
        "\n",
        "---\n",
        "\n",
        "###  Reflexión final\n",
        "\n",
        "La documentación de ambos proyectos demuestra dos filosofías distintas:  \n",
        "- AWS Glue prioriza la **automatización y gobernanza gestionada**.  \n",
        "- Apache Airflow prioriza la **flexibilidad y estandarización abierta**.  \n",
        "\n",
        "Esta complementariedad refleja la evolución natural del ecosistema de datos:  \n",
        "la orquestación moderna no depende de una única herramienta, sino de la capacidad de integrarlas eficientemente dentro de una arquitectura escalable y reproducible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8c466c1",
      "metadata": {
        "id": "f8c466c1"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "1. AWS Docs. \"Tutorial: Adding an AWS Glue crawler\". URL: https://docs.aws.amazon.com/glue/latest/dg/tutorial-add-crawler.html (Accedido: 3-Nov-2025).\n",
        "2. AWS Big Data Blog (Noritaka Sekiyama). \"End-to-end development lifecycle for data engineers to build a data integration pipeline using AWS Glue\". URL: https://aws.amazon.com/blogs/big-data/end-to-end-development-lifecycle-for-data-engineers-to-build-a-data-integration-pipeline-using-aws-glue (Publicado: 26-Jul-2023).\n",
        "3. AWS Big Data Blog (Leonardo Gomez). \"Making ETL easier with AWS Glue Studio\". URL: https://aws.amazon.com/es/blogs/big-data/making-etl-easier-with-aws-glue-studio/\n",
        " (Publicado: 24-Sep-2020)\n",
        "4. Apache Airflow Docs. \"Building a Simple Data Pipeline\" (Contenido versión 3.1.2). URL: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/pipeline.html (Accedido: 3-Nov-2025).\n",
        "5. Apache Airflow Docs. \"Pythonic Dags with the TaskFlow API\" (Contenido versión 3.1.2). URL: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html (Accedido: 3-Nov-2025).\n",
        "6. Apache Airflow Docs.  \"UI Overview\" — (Coontenido versión 3.1.2). URL: https://airflow.apache.org/docs/apache-airflow/3.1.2/ui.html (Accedido: 3-Nov-2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37f97caf",
      "metadata": {
        "id": "37f97caf"
      },
      "source": [
        "# 8. Autores y Contribuciones\n",
        "\n",
        "- **Autor(es):** Ignacio Ramírez, Antonia Montecinos, Cristian Vergara\n",
        "- **Contribuciones:**\n",
        "  - [todos]: Jefe de Proyecto y Redactor Principal. Investigación y redacción de secciones 4, 5, 6 y 7.\n",
        "  - [todos]: Diseñador. Búsqueda de ejemplos de terceros y creación de la presentación.\n",
        "  - [todos]: Revisor técnico. Validación de criterios y referencias."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gX9vOcZF7C8w",
      "metadata": {
        "id": "gX9vOcZF7C8w"
      },
      "source": [
        "## Checklist de entrega (marcar antes de subir):\n",
        "- [x] Documento (notebook o PDF) con la investigación y análisis comparativo.\n",
        "- [x] Referencias completas a los ejemplos de terceros usados (link, autor/organización, fecha, breve descripción de uso).\n",
        "- [x] Evidencia visual o descriptiva de los ejemplos (capturas, tablas, o explicación detallada).\n",
        "- [x] Archivo ZIP con recursos adicionales (opcional: datos, scripts) o enlace al repositorio fuente.\n",
        "- [x] Presentación (diapositivas) o PDF para exponer en clase.\n",
        "- [x] Autor(es) y contribuciones (lista de integrantes y roles — quién hizo qué).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
