# Pipeline de IngestiÃ³n de Datos Grandes (>= 200 MB) desde la Nube

Este directorio contiene el notebook `Prueba1.ipynb`, que demuestra **estrategias prÃ¡cticas para trabajar con datasets grandes sin descargarlos completamente al disco local**. El foco es educativo y se orienta a comparar enfoques y herramientas (DuckDB, Pandas, Dask, streaming manual) en escenarios de mÃ¡s de 200 MB de datos remotos.

## âœ… Objetivos del Notebook
- Leer mÃºltiples archivos **Parquet remotos** (NYC Yellow Taxi) superando 200 MB combinados sin almacenarlos localmente.
- Aplicar **consultas SQL con DuckDB** sobre archivos en la nube (HTTP range requests, predicate pushdown, column projection).
- Mostrar un **patrÃ³n de streaming por chunks** para CSV remoto grande usando `requests` + `pandas`.
- Comparar **Pandas vs Dask**: memoria, paralelismo, modelo de ejecuciÃ³n, rendimiento y cuÃ¡ndo elegir cada uno.
- Ejecutar un **benchmark reproducible** (agregaciones) para ilustrar overhead y escalabilidad.

---
## ðŸ—‚ Dataset Principal
**NYC Yellow Taxi Trip Records** (CloudFront CDN):
- Base URL: `https://d37ci6vzurychx.cloudfront.net/trip-data/`
- Formato: `yellow_tripdata_YYYY-MM.parquet`
- TamaÃ±o por archivo: ~180â€“250 MB
- Ejemplo usado: meses `2024-01`, `2024-02`, `2024-03` (conjunto > 500 MB lÃ³gicos).

> No se descargan completos; DuckDB y Dask leen sÃ³lo lo necesario (lectura columnar + HTTP range requests).

### Columnas Relevantes Usadas
- `tpep_pickup_datetime`
- `trip_distance`
- `total_amount`
- `passenger_count`

---
## ðŸ§° Herramientas y Roles
| Herramienta | Rol Principal | Ventajas Clave |
|-------------|--------------|----------------|
| **DuckDB** | SQL local sobre datos remotos (Parquet) | Pushdown de filtros y columnas; sintaxis SQL familiar |
| **Pandas** | ExploraciÃ³n y manipulaciÃ³n en memoria | API madura, ideal si cabe en RAM |
| **Dask DataFrame** | Escalar DataFrames mayores que RAM | EjecuciÃ³n lazy, paralelismo multi-core, mÃºltiples archivos |
| **requests** | Streaming manual HTTP | Control detallado chunk a chunk |
| **PyArrow** | Backend Parquet/columnar | Eficiencia en lectura de columnas |
| **fsspec** | Acceso unificado a sistemas de archivos | Soporte HTTP/S3 sin copiar local |

---
## âš™ï¸ Dependencias
Se instalan dinÃ¡micamente en el notebook si faltan:
```
duckdb
pyarrow
fsspec
datasets
pandas
dask[dataframe]
requests
```

### InstalaciÃ³n manual (opcional) en PowerShell
```powershell
# (Recomendado) Crear entorno virtual
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Actualizar pip
python -m pip install --upgrade pip

# Instalar dependencias
python -m pip install duckdb pyarrow fsspec datasets pandas "dask[dataframe]" requests
```

> Si aparece el error de polÃ­ticas de ejecuciÃ³n al activar, ejecutar:
```powershell
Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy RemoteSigned
```

---
## â–¶ï¸ Flujo de Trabajo en el Notebook
1. **SelecciÃ³n de URLs Parquet**: ConstrucciÃ³n de la lista de meses a consultar.
2. **Consulta inicial con DuckDB**: `read_parquet([...])` + `LIMIT` para muestra.
3. **Agregaciones**: Group-by diario aplicando filtros (`WHERE trip_distance BETWEEN 0.1 AND 100`).
4. **InspecciÃ³n de esquema** sin leer datos: `DESCRIBE SELECT * FROM read_parquet('...') LIMIT 0`.
5. **Streaming CSV**: Lectura progresiva con `requests.iter_lines()` y acumulaciÃ³n chunk a chunk.
6. **Dask Parquet**: Carga lazy de patrÃ³n (`yellow_tripdata_2024-0*.parquet`) y agregaciÃ³n.
7. **Dask CSV**: Lectura con `blocksize` configurado.
8. **Resumen comparativo (Markdown)**: Diferencias Pandas vs Dask.
9. **Benchmark**: Tiempos de agregaciÃ³n Pandas vs Dask (subset de filas) para ilustrar overhead.

---
## ðŸ§ª Benchmark Incluido
El benchmark realiza:
- Pandas: extracciÃ³n de 300k filas con DuckDB + `groupby`.
- Dask: misma agregaciÃ³n sobre DataFrame lazy (limitando con `head()` para volumen comparable).
- Repite 3 veces y calcula medias y desviaciones.

### InterpretaciÃ³n TÃ­pica
| Caso | Resultado ComÃºn |
|------|-----------------|
| Subconjunto pequeÃ±o | Pandas suele ser mÃ¡s rÃ¡pido (menor overhead) |
| Volumen grande / muchos archivos | Dask escala mejor al paralelizar |
| Pre-filtrado SQL antes de Pandas/Dask | Acelera ambas rutas; reduce memoria |

> Ajustar el `LIMIT` o eliminarlo para observar cuÃ¡ndo Dask empieza a superar a Pandas.

---
## ðŸ§  Pandas vs Dask (Resumen Conceptual)
| DimensiÃ³n | Pandas | Dask |
|----------|--------|------|
| EjecuciÃ³n | Inmediata | Lazy (DAG) |
| Escalado | RAM local | Multi-core / cluster |
| Lectura mÃºltiple Parquet | Manual (lista + concat) | PatrÃ³n con wildcard | 
| Control Memoria | Limitado (chunks CSV) | Particiones automÃ¡ticas |
| Overhead | Bajo | Scheduler (~ms) |
| Ideal Para | ExploraciÃ³n rÃ¡pida | ETL repetible, big data |

**Complemento**: Usar DuckDB antes de Pandas/Dask para minimizar el volumen.

---
## ðŸ›  Patrones Clave del CÃ³digo
### 1. Consulta remota Parquet (DuckDB)
```python
query = f"""
SELECT date_trunc('day', tpep_pickup_datetime) AS pickup_date,
       passenger_count, trip_distance, total_amount
FROM read_parquet([{','.join([repr(u) for u in urls])}])
WHERE trip_distance > 0
LIMIT 1000
"""
df_sample = duckdb.query(query).to_df()
```
### 2. AgregaciÃ³n remota
```python
agg_query = f"""
SELECT date_trunc('day', tpep_pickup_datetime) AS day,
       COUNT(*) AS trips,
       AVG(trip_distance) AS avg_distance,
       AVG(total_amount) AS avg_amount
FROM read_parquet([{','.join([repr(u) for u in urls])}])
WHERE trip_distance BETWEEN 0.1 AND 100
GROUP BY 1 ORDER BY 1 LIMIT 15
"""
df_agg = duckdb.query(agg_query).to_df()
```
### 3. Streaming CSV manual
```python
with requests.get(csv_url, stream=True) as r:
    for i, line in enumerate(r.iter_lines(decode_unicode=True)):
        # acumulaciÃ³n y parseo por bloques
```
### 4. Dask Parquet
```python
pattern = base_url + '/yellow_tripdata_2024-0*.parquet'
ddf = dd.read_parquet(pattern, engine='pyarrow', gather_statistics=False)
```
### 5. Benchmark (extracto)
```python
pdf = duckdb.query(SQL_SUBSET).to_df()
agg_pdf = pdf.groupby('pickup_date').agg({...})
# vs
result = ddf.assign(pickup_date=...).groupby('pickup_date').agg({...}).head(20).compute()
```

---
## ðŸš€ CÃ³mo Ejecutar el Notebook
1. Abrir VS Code y este directorio.
2. (Opcional) Activar entorno virtual.
3. Abrir `Prueba1.ipynb` y ejecutar celdas secuencialmente (algunas celdas instalan dependencias si faltan).
4. Revisar la celda de **benchmark** al final para observar tiempos.

### Consejos de EjecuciÃ³n
- Si la red es lenta, aumentar el `LIMIT` gradualmente.
- Para un anÃ¡lisis mÃ¡s profundo, retirar `LIMIT` de consultas y observar consumo.
- Si aparece error de columna ausente, verificar nombre (`tpep_pickup_datetime`).

---
## ðŸ” Posibles Extensiones
| Mejora | DescripciÃ³n |
|--------|-------------|
| Polars | AÃ±adir comparaciÃ³n adicional (lazy + SIMD). |
| Persistencia Dask | AÃ±adir `.persist()` tras filtrado para reutilizar resultados. |
| MÃ©tricas Memoria | Integrar mediciÃ³n con `psutil` o `memory_profiler`. |
| Visual DAG | Usar `ddf.visualize()` para enseÃ±ar el grafo de tareas. |
| MÃ¡s Meses | Incluir mÃ¡s meses y observar escalado. |
| Joins complejos | Unir con tablas de tarifas o zonas para enriquecer anÃ¡lisis. |

---
## â“ FAQ RÃ¡pido
**Â¿Por quÃ© DuckDB si ya uso Dask?**
Para filtrar/seleccionar columnas tempranamente con SQL eficiente y reducir bytes leÃ­dos.

**Â¿Necesito descargar los Parquet primero?**
No. DuckDB y PyArrow pueden hacer HTTP range requests.

**Â¿Dask siempre serÃ¡ mÃ¡s rÃ¡pido?**
No: en volÃºmenes pequeÃ±os el overhead lo hace mÃ¡s lento que Pandas.

---
## âš–ï¸ DecisiÃ³n de Herramienta (GuÃ­a Express)
| SituaciÃ³n | Elige |
|-----------|-------|
| Dataset cabe en RAM y exploraciÃ³n ad-hoc | Pandas |
| Muchos archivos mensuales (GB totales) y pipeline repetible | Dask |
| Necesitas SQL y reducciÃ³n previa sin cluster | DuckDB |
| Flujos lineales streaming (CSV/JSON) | requests + chunks |

---
## ðŸ“ Licenciamiento / Uso de Datos
Los datos de NYC Taxi tienen tÃ©rminos pÃºblicos para uso educativo (ver documentaciÃ³n oficial). Verifica siempre restricciones antes de redistribuir.

---
## ðŸ“Œ Resumen Final
Este notebook demuestra un **enfoque hÃ­brido**: combinar SQL columnar (DuckDB) + DataFrames (Pandas/Dask) + streaming manual para abordar datasets que exceden memoria sin infraestructura compleja. La elecciÃ³n de herramienta depende del tamaÃ±o, patrÃ³n de acceso y requisitos de reproducibilidad.

> Sugerencia: Usa este README como guÃ­a de laboratorio para experimentos incrementales. Empieza con 1 mes, escala a 6â€“12 meses y compara tiempos/memoria registrando tus observaciones.
