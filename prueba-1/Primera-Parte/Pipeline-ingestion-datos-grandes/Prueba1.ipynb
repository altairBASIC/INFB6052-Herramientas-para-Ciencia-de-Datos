{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137032d2",
   "metadata": {},
   "source": [
    "# Pipeline de ingestión de datos grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db87b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Trabajo con dataset >200MB directamente desde la nube (sin guardar local) ===\n",
    "# En este notebook mostraremos varias estrategias:\n",
    "# 1. Consultar múltiples archivos Parquet remotos grandes (NYC Taxi Trips) usando DuckDB sin descargarlos por completo.\n",
    "# 2. Patrones de procesamiento en streaming / por chunks para CSV remoto grande.\n",
    "# 3. Streaming con Hugging Face Datasets (sin descargar al disco) como demostración opcional.\n",
    "#\n",
    "# Requisito: No almacenar el archivo completo en disco local. Las herramientas (DuckDB, fsspec, datasets) permiten acceso lazy.\n",
    "#\n",
    "# ---- Dataset principal elegido ----\n",
    "# NYC Yellow Taxi Trip Records (CloudFront CDN):\n",
    "#   URL base: https://d37ci6vzurychx.cloudfront.net/trip-data/\n",
    "#   Archivos mensuales: yellow_tripdata_YYYY-MM.parquet (≈180–250 MB cada uno).\n",
    "# Procesaremos varios meses hasta superar 200 MB (p.ej. 2024-01, 2024-02, 2024-03).\n",
    "#\n",
    "# Ventajas de usar Parquet remoto + DuckDB:\n",
    "# - DuckDB puede hacer predicate pushdown y column projection, leyendo sólo las columnas necesarias.\n",
    "# - Evitamos descargar todo el archivo (lee fragmentos vía HTTP range requests).\n",
    "#\n",
    "# Si deseas otros datasets grandes: Amazon Reviews (parquet en S3), OpenSky, LA Taxi, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\OneDrive\\Documentos\\vscode prueba\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versiones -> duckdb: 1.4.1 pyarrow: 21.0.0\n"
     ]
    }
   ],
   "source": [
    "# Instalación (idempotente) de librerías necesarias para acceso remoto\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def ensure(pkg):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
    "\n",
    "for p in ['duckdb', 'pyarrow', 'fsspec', 'datasets']:  # datasets para streaming HF\n",
    "    ensure(p)\n",
    "\n",
    "import duckdb, pyarrow as pa, pyarrow.dataset as ds\n",
    "print('Versiones -> duckdb:', duckdb.__version__, 'pyarrow:', pa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff1eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos remotos seleccionados:\n",
      " - https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
      " - https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet\n",
      " - https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet\n",
      "Ejecutando consulta...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1.72</td>\n",
       "      <td>22.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>18.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4.70</td>\n",
       "      <td>31.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1.40</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>16.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pickup_date  passenger_count  trip_distance  total_amount\n",
       "0  2024-01-01                1           1.72         22.70\n",
       "1  2024-01-01                1           1.80         18.75\n",
       "2  2024-01-01                1           4.70         31.30\n",
       "3  2024-01-01                1           1.40         17.00\n",
       "4  2024-01-01                1           0.80         16.10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Consulta remota de varios Parquet (>200MB total) con DuckDB (lazy scan)\n",
    "import duckdb, pandas as pd\n",
    "\n",
    "base_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data'\n",
    "months = ['2024-01', '2024-02', '2024-03']  # 3 meses ~ >500MB combinados\n",
    "urls = [f\"{base_url}/yellow_tripdata_{m}.parquet\" for m in months]\n",
    "print('Archivos remotos seleccionados:')\n",
    "for u in urls: print(' -', u)\n",
    "\n",
    "# Nota: El esquema usa tpep_pickup_datetime (no pickup_datetime)\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  date_trunc('day', tpep_pickup_datetime) AS pickup_date,\n",
    "  passenger_count,\n",
    "  trip_distance,\n",
    "  total_amount\n",
    "FROM read_parquet([{','.join([repr(u) for u in urls])}])\n",
    "WHERE trip_distance > 0\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "print('Ejecutando consulta...')\n",
    "df_sample = duckdb.query(query).to_df()\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando agregaciones...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>trips</th>\n",
       "      <th>avg_distance</th>\n",
       "      <th>avg_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>19.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5.725000</td>\n",
       "      <td>36.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>10</td>\n",
       "      <td>2.601000</td>\n",
       "      <td>22.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>76312</td>\n",
       "      <td>4.296120</td>\n",
       "      <td>30.149872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>73697</td>\n",
       "      <td>4.189915</td>\n",
       "      <td>30.146731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>80709</td>\n",
       "      <td>3.826030</td>\n",
       "      <td>28.556487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>101058</td>\n",
       "      <td>3.371255</td>\n",
       "      <td>27.160826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>101089</td>\n",
       "      <td>3.244567</td>\n",
       "      <td>26.380395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-01-06</td>\n",
       "      <td>94634</td>\n",
       "      <td>3.206192</td>\n",
       "      <td>25.013898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>65831</td>\n",
       "      <td>3.983030</td>\n",
       "      <td>27.921213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>78457</td>\n",
       "      <td>3.581777</td>\n",
       "      <td>27.690064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>91166</td>\n",
       "      <td>2.928762</td>\n",
       "      <td>25.206813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>93161</td>\n",
       "      <td>3.254472</td>\n",
       "      <td>26.813976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-01-11</td>\n",
       "      <td>102904</td>\n",
       "      <td>3.330293</td>\n",
       "      <td>27.640530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          day   trips  avg_distance  avg_amount\n",
       "0  2002-12-31       4      2.000000   10.787500\n",
       "1  2008-12-31       1      1.620000   19.900000\n",
       "2  2009-01-01       4      5.725000   36.212500\n",
       "3  2023-12-31      10      2.601000   22.462000\n",
       "4  2024-01-01   76312      4.296120   30.149872\n",
       "5  2024-01-02   73697      4.189915   30.146731\n",
       "6  2024-01-03   80709      3.826030   28.556487\n",
       "7  2024-01-04  101058      3.371255   27.160826\n",
       "8  2024-01-05  101089      3.244567   26.380395\n",
       "9  2024-01-06   94634      3.206192   25.013898\n",
       "10 2024-01-07   65831      3.983030   27.921213\n",
       "11 2024-01-08   78457      3.581777   27.690064\n",
       "12 2024-01-09   91166      2.928762   25.206813\n",
       "13 2024-01-10   93161      3.254472   26.813976\n",
       "14 2024-01-11  102904      3.330293   27.640530"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.b Agregaciones (sólo lee columnas necesarias)\n",
    "agg_query = f\"\"\"\n",
    "SELECT\n",
    "  date_trunc('day', tpep_pickup_datetime) AS day,\n",
    "  COUNT(*) AS trips,\n",
    "  AVG(trip_distance) AS avg_distance,\n",
    "  AVG(total_amount) AS avg_amount\n",
    "FROM read_parquet([{','.join([repr(u) for u in urls])}])\n",
    "WHERE trip_distance BETWEEN 0.1 AND 100 -- filtra outliers y fuerza predicate pushdown\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "LIMIT 15\n",
    "\"\"\"\n",
    "print('Ejecutando agregaciones...')\n",
    "df_agg = duckdb.query(agg_query).to_df()\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0573a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando stream (simulado) y procesando por chunks...\n",
      "Filas procesadas: 221 | Suma Count: 0\n",
      "Filas procesadas: 221 | Suma Count: 0\n"
     ]
    }
   ],
   "source": [
    "# 2. Patrón de lectura por chunks (streaming) para CSV remoto grande\n",
    "# Ejemplo: usaremos un CSV público relativamente grande. Si no está disponible, mostrar fallback.\n",
    "import pandas as pd\n",
    "import io, requests\n",
    "\n",
    "csv_url = 'https://raw.githubusercontent.com/plotly/datasets/master/2011_february_us_airport_traffic.csv'  # reemplazar por uno más grande si se desea\n",
    "chunk_size = 5000\n",
    "acc_rows = 0\n",
    "sum_metric = 0\n",
    "\n",
    "print('Descargando stream (simulado) y procesando por chunks...')\n",
    "try:\n",
    "    with requests.get(csv_url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        buffer = io.StringIO()\n",
    "        for i, line in enumerate(r.iter_lines(decode_unicode=True)):\n",
    "            if line is None:\n",
    "                continue\n",
    "            buffer.write(line + '\\n')\n",
    "            # Procesar cada N líneas acumuladas (chunk lógico)\n",
    "            if (i+1) % chunk_size == 0:\n",
    "                buffer.seek(0)\n",
    "                df_chunk = pd.read_csv(buffer)\n",
    "                if 'Count' in df_chunk.columns:\n",
    "                    sum_metric += df_chunk['Count'].sum()\n",
    "                acc_rows += len(df_chunk)\n",
    "                buffer = io.StringIO()\n",
    "        # Procesar resto\n",
    "        buffer.seek(0)\n",
    "        remaining = buffer.getvalue().strip()\n",
    "        if remaining:\n",
    "            df_chunk = pd.read_csv(io.StringIO(remaining))\n",
    "            if 'Count' in df_chunk.columns:\n",
    "                sum_metric += df_chunk['Count'].sum()\n",
    "            acc_rows += len(df_chunk)\n",
    "    print('Filas procesadas:', acc_rows, '| Suma Count:', sum_metric)\n",
    "except Exception as e:\n",
    "    print('Fallo streaming CSV:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mostrando esquema del primer archivo remoto (sin leer datos completos)...\n",
      "Columnas y tipos detectados (primeros 30):\n",
      "Columnas y tipos detectados (primeros 30):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>column_type</th>\n",
       "      <th>null</th>\n",
       "      <th>key</th>\n",
       "      <th>default</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VendorID</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tpep_pickup_datetime</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tpep_dropoff_datetime</td>\n",
       "      <td>TIMESTAMP</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>passenger_count</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trip_distance</td>\n",
       "      <td>DOUBLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RatecodeID</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>store_and_fwd_flag</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PULocationID</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DOLocationID</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>payment_type</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fare_amount</td>\n",
       "      <td>DOUBLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>extra</td>\n",
       "      <td>DOUBLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mta_tax</td>\n",
       "      <td>DOUBLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tip_amount</td>\n",
       "      <td>DOUBLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tolls_amount</td>\n",
       "      <td>DOUBLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>improvement_surcharge</td>\n",
       "      <td>DOUBLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>total_amount</td>\n",
       "      <td>DOUBLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>congestion_surcharge</td>\n",
       "      <td>DOUBLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Airport_fee</td>\n",
       "      <td>DOUBLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              column_name column_type null   key default extra\n",
       "0                VendorID     INTEGER  YES  None    None  None\n",
       "1    tpep_pickup_datetime   TIMESTAMP  YES  None    None  None\n",
       "2   tpep_dropoff_datetime   TIMESTAMP  YES  None    None  None\n",
       "3         passenger_count      BIGINT  YES  None    None  None\n",
       "4           trip_distance      DOUBLE  YES  None    None  None\n",
       "5              RatecodeID      BIGINT  YES  None    None  None\n",
       "6      store_and_fwd_flag     VARCHAR  YES  None    None  None\n",
       "7            PULocationID     INTEGER  YES  None    None  None\n",
       "8            DOLocationID     INTEGER  YES  None    None  None\n",
       "9            payment_type      BIGINT  YES  None    None  None\n",
       "10            fare_amount      DOUBLE  YES  None    None  None\n",
       "11                  extra      DOUBLE  YES  None    None  None\n",
       "12                mta_tax      DOUBLE  YES  None    None  None\n",
       "13             tip_amount      DOUBLE  YES  None    None  None\n",
       "14           tolls_amount      DOUBLE  YES  None    None  None\n",
       "15  improvement_surcharge      DOUBLE  YES  None    None  None\n",
       "16           total_amount      DOUBLE  YES  None    None  None\n",
       "17   congestion_surcharge      DOUBLE  YES  None    None  None\n",
       "18            Airport_fee      DOUBLE  YES  None    None  None"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda extra: inspección de esquema remoto (lee sólo metadatos)\n",
    "import duckdb\n",
    "print('Mostrando esquema del primer archivo remoto (sin leer datos completos)...')\n",
    "# PRAGMA table_info no acepta directamente una table function; usamos DESCRIBE SELECT\n",
    "info_df = duckdb.query(f\"DESCRIBE SELECT * FROM read_parquet('{urls[0]}') LIMIT 0\").to_df()\n",
    "print('Columnas y tipos detectados (primeros 30):')\n",
    "info_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad169f44",
   "metadata": {},
   "source": [
    "## Comparativa Pandas vs Dask para Lectura y Manipulación de Datos\n",
    "\n",
    "Objetivos:\n",
    "- Mostrar enfoques de lectura de datos grandes sin cargar todo en memoria.\n",
    "- Comparar patrón de chunks manual en Pandas vs carga perezosa/particionada en Dask.\n",
    "- Efectuar operaciones típicas: filtrado, agregados y cálculo de métricas.\n",
    "- Usar dataset remoto (NYC Yellow Taxi Parquet) y un ejemplo CSV (stream) para chunks.\n",
    "\n",
    "Resumen conceptual:\n",
    "- **Pandas**: Opera en memoria; soporta `chunksize` en `read_csv` pero no en `read_parquet` directamente. Para Parquet se puede usar PyArrow / RowGroups manual.\n",
    "- **Dask DataFrame**: Abstracción paralela de DataFrames (“colección” de particiones de Pandas). Lee múltiples archivos (`read_parquet`, `read_csv`) y ejecuta un DAG; computa solo al llamar `.compute()`.\n",
    "- Caso de uso: Agregaciones diarias y métricas condicionales sobre millones de filas sin traer todo a memoria a la vez.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación/verificación Dask\n",
    "import importlib, sys, subprocess\n",
    "\n",
    "def ensure(pkg):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        print('Instalando', pkg)\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
    "\n",
    "for p in ['dask[dataframe]']:\n",
    "    ensure(p.split('[')[0])\n",
    "\n",
    "import dask, dask.dataframe as dd\n",
    "print('Version Dask:', dask.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0038c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask: lectura perezosa de múltiples Parquet remotos\n",
    "# Nota: Dask puede necesitar fsspec + s3fs/httpfs; ya instalamos fsspec.\n",
    "import dask.dataframe as dd\n",
    "\n",
    "parquet_pattern = base_url + '/yellow_tripdata_2024-0*.parquet'  # patrón para meses 01..09 (aquí aplican 01,02,03 disponibles)\n",
    "print('Leyendo patrón remoto con Dask:', parquet_pattern)\n",
    "# engine=pyarrow para mejor compatibilidad\n",
    "ddf = dd.read_parquet(parquet_pattern, engine='pyarrow', gather_statistics=False)\n",
    "print(ddf)\n",
    "\n",
    "# Seleccionar columnas necesarias (optimiza graph)\n",
    "cols_needed = ['tpep_pickup_datetime', 'trip_distance', 'total_amount', 'passenger_count']\n",
    "existing = [c for c in cols_needed if c in ddf.columns]\n",
    "ddf_small = ddf[existing]\n",
    "\n",
    "# Agregación diaria (lazy)\n",
    "ddf_small['pickup_date'] = ddf_small['tpep_pickup_datetime'].dt.date\n",
    "agg_dd = ddf_small.groupby('pickup_date').agg({\n",
    "    'trip_distance':'mean',\n",
    "    'total_amount':'mean',\n",
    "    'passenger_count':'mean'\n",
    "}).rename(columns={\n",
    "    'trip_distance':'avg_distance',\n",
    "    'total_amount':'avg_amount',\n",
    "    'passenger_count':'avg_passengers'\n",
    "})\n",
    "\n",
    "print('Computando agregación Dask...')\n",
    "result_dd = agg_dd.head(15).compute()  # head primero reduce costo\n",
    "result_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparativa CSV grande: Pandas chunks vs Dask\n",
    "# Reutilizamos csv_url (puedes sustituir por un CSV más grande público)\n",
    "import dask.dataframe as dd\n",
    "\n",
    "print('Leyendo CSV remoto con Dask (lazy)...')\n",
    "ddf_csv = dd.read_csv(csv_url, blocksize=256_000)  # blocksize ~256KB ejemplo\n",
    "print(ddf_csv)\n",
    "\n",
    "# Ejemplo filtrado y agregado (si existe la columna 'Count')\n",
    "if 'Count' in ddf_csv.columns:\n",
    "    dask_csv_metric = ddf_csv['Count'].mean().compute()\n",
    "    print('Dask mean Count:', dask_csv_metric)\n",
    "else:\n",
    "    print('Columna Count no encontrada en CSV remoto para Dask ejemplo.')\n",
    "\n",
    "# Para comparar: ya calculamos sum_metric en Pandas (chunks). Podríamos normalizar:\n",
    "# (suma / filas) equivalente a media si la queremos; aquí dejamos referencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c3db90",
   "metadata": {},
   "source": [
    "### Resumen Comparativo Pandas vs Dask\n",
    "\n",
    "| Aspecto | Pandas | Dask |\n",
    "|---------|--------|------|\n",
    "| Modelo de ejecución | Inmediato, en memoria | Lazy (DAG), paralelo |\n",
    "| Escalado | Memoria RAM de una sola máquina | Multi-core/local cluster; escala mejor |\n",
    "| Lectura Parquet múltiple | Necesita manejar lista y concat; sin lazy row-group by default | `read_parquet` con comodín/patrón, particiones distribuidas |\n",
    "| Chunks CSV | `read_csv(chunksize=...)` (iterador manual) | `read_csv` crea particiones; operaciones vectorizadas sobre todas |\n",
    "| Optimizaciones | Vectorización y C internals | Task graph, predicate pushdown parcial vía motores subyacentes |\n",
    "| Cuándo elegir | Datos caben en RAM y prototipado rápido | Datos grandes/borderline RAM, pipelines repetibles, paralelismo |\n",
    "\n",
    "Puntos clave:\n",
    "- Para datasets > memoria, Dask evita OOM ejecutando en particiones.\n",
    "- Pandas chunks requiere lógica manual (acumulación de agregados). Dask ofrece API similar a Pandas con `.compute()` al final.\n",
    "- DuckDB complementa ambos cuando se necesitan consultas SQL/pushdown sobre archivos columnares remotos.\n",
    "\n",
    "Recomendación práctica:\n",
    "1. Empezar con Pandas para exploración rápida.\n",
    "2. Migrar a Dask si: (a) RAM insuficiente, (b) se requieren múltiples archivos grandes, (c) necesidad de paralelizar.\n",
    "3. Integrar DuckDB para consultas ad-hoc y reducir volumen antes de pasar a Pandas/Dask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf3590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f96972dc",
   "metadata": {},
   "source": [
    "## Pandas vs Dask: Funcionalidad, Ventajas, Limitaciones y Rendimiento\n",
    "\n",
    "### 1. Funcionalidad Básica\n",
    "- **Pandas**: Librería de manipulación de datos en memoria (DataFrame/Series) con operaciones vectorizadas, rica API (groupby, merges, reshape) y excelente integración con ecosistema científico (NumPy, scikit-learn, matplotlib). Ideal para exploración interactiva y datasets que caben en RAM.\n",
    "- **Dask DataFrame**: Capa distribuida/perezosa que parte un dataset grande en múltiples particiones (cada una un DataFrame Pandas) y construye un *DAG* de tareas. Solo ejecuta (descarga/lee/computa) cuando se llama a `.compute()` (o métodos que disparan acción). Permite escalar a datos mayores que la memoria y aprovechar múltiples núcleos.\n",
    "\n",
    "### 2. Modelo de Memoria\n",
    "| Aspecto | Pandas | Dask |\n",
    "|---------|--------|------|\n",
    "| Carga inicial | Trae el dataset completo (salvo iteradores CSV con `chunksize`) | Crea un plan (DAG) y lectura diferida por partición |\n",
    "| Memoria pico | Aproximadamente tamaño del dataset (más overhead de estructuras) | Depende de tamaño de partición; puede procesar en bloques y liberar |\n",
    "| Control granular | Requiere manualmente `chunksize` / filtrado previo | Divisiones automáticas; puede persistir solo pasos intermedios críticos |\n",
    "| Evitar OOM | Difícil si el dataset completo no cabe | Procesa por partición; se puede ajustar el tamaño de bloque |\n",
    "\n",
    "### 3. Paralelismo y Ejecución\n",
    "| Tema | Pandas | Dask |\n",
    "|------|--------|------|\n",
    "| Paralelismo CPU | Limitado (Global Interpreter Lock) salvo operaciones en C internas | Paraleliza entre particiones (multi-core) y puede ir a cluster |\n",
    "| Lazy vs Eager | Eager (inmediato) | Lazy (optimiza y ejecuta al final) |\n",
    "| Overhead | Mínimo para operaciones simples | Overhead de scheduler (≈10–50 ms + planificación) por cómputo |\n",
    "| Escenarios ideales | Análisis rápido, prototipos, transformaciones intensivas pero medianas | ETL de muchos archivos, pipelines repetibles, datos >RAM, integración con cluster |\n",
    "\n",
    "### 4. Facilidad de Uso\n",
    "- Pandas tiene curva de aprendizaje más corta y más documentación/examples.\n",
    "- Dask replica gran parte de la API de Pandas, pero exige entender diferencias: ejecuciones diferidas, `.compute()`, particiones, efectos de persistencia/caché.\n",
    "- Debug: en Pandas se inspecciona inmediatamente; en Dask hay que revisar el DAG ( `.visualize()` ) y a veces forzar materialización parcial.\n",
    "\n",
    "### 5. Rendimiento (Observaciones Generales)\n",
    "| Situación | Qué suele ganar | Razón |\n",
    "|-----------|-----------------|-------|\n",
    "| Dataset pequeño (<200 MB) | Pandas | Menor overhead; todo cabe en RAM |\n",
    "| Muchos archivos Parquet/CSV en un patrón | Dask | Lectura distribuida y pushdown columnar por partición |\n",
    "| Fusión / groupby grande que casi llena RAM | Dask (si bien configurado) | Procesa por particiones y reduce uso pico de memoria |\n",
    "| Operaciones muy vectorizadas en columnas numéricas moderadas | Pandas | Menos capas; ejecución directa en C/NumPy |\n",
    "| Pipeline escalable / reejecutable en batch | Dask | DAG reproducible y escalable a cluster |\n",
    "\n",
    "### 6. Limitaciones\n",
    "| Pandas | Dask |\n",
    "|--------|------|\n",
    "| No escala fuera de RAM sin trabajo manual (particionamiento propio) | Algunas funciones Pandas no están 100% implementadas; requiere cuidado en merges grandes |\n",
    "| Operaciones paralelas limitadas | Overhead puede volverlo más lento en datasets pequeños |\n",
    "| Riesgo de OOM en joins / groupbys grandes | Requiere tuning (tamaño de partición, persist, scheduler) |\n",
    "\n",
    "### 7. Integración con Otros Enfoques\n",
    "- **DuckDB** + Pandas: Pre-filtrar y seleccionar columnas desde archivos Parquet remotos y luego pasar un subconjunto a Pandas/Dask.\n",
    "- **Dask + scikit-learn**: Para grandes datos tabulares, usar `dask-ml` o convertir a muestras representativas.\n",
    "- **Polars** (mención rápida): Otra alternativa columnar/rápida (no incluida en demo) que mezcla lazy + eager.\n",
    "\n",
    "### 8. Recomendación de Estrategia\n",
    "1. Usar DuckDB o filtros de origen para reducir columnas/filas iniciales.\n",
    "2. Si el resultado reducido cabe en memoria → Pandas.\n",
    "3. Si no cabe o el pipeline recorrerá muchos archivos cada día → Dask.\n",
    "4. Medir con un subconjunto primero (1–5%) para calibrar costos.\n",
    "\n",
    "La siguiente celda incluye benchmarks reproducibles (lectura Parquet parcial y agregación) comparando Pandas (vía DuckDB extracción limitada) vs Dask (paralelo). Ejecuta y observa los tiempos en tu entorno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943410b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarks comparativos (tiempos) Pandas (vía DuckDB para extraer subset) vs Dask\n",
    "import time, duckdb, pandas as pd, dask.dataframe as dd\n",
    "from statistics import mean\n",
    "\n",
    "# Reutilizamos variables: base_url, urls ya definidos antes (meses 2024-01..03)\n",
    "# 1. Pandas: extraer un subconjunto con DuckDB (simulando filtrado previo) y luego groupby en Pandas\n",
    "pandas_times = []\n",
    "for i in range(3):\n",
    "    t0 = time.perf_counter()\n",
    "    # Leer solo columnas necesarias y cierta ventana (limit) para aislar costo lógico\n",
    "    pdf = duckdb.query(f\"SELECT tpep_pickup_datetime, trip_distance, total_amount, passenger_count FROM read_parquet([{','.join([repr(u) for u in urls])}]) WHERE trip_distance > 0 LIMIT 300000\").to_df()\n",
    "    # Groupby Pandas\n",
    "    pdf['pickup_date'] = pdf['tpep_pickup_datetime'].dt.date\n",
    "    agg_pdf = pdf.groupby('pickup_date', as_index=False).agg({\n",
    "        'trip_distance':'mean', 'total_amount':'mean', 'passenger_count':'mean'\n",
    "    })\n",
    "    pandas_times.append(time.perf_counter()-t0)\n",
    "\n",
    "# 2. Dask: usar DataFrame completo (lazy) y computar la misma agregación (head para similar volumen)\n",
    "dask_times = []\n",
    "# Construimos solo si no existe\n",
    "ddf_cols = ['tpep_pickup_datetime','trip_distance','total_amount','passenger_count']\n",
    "if 'ddf' in globals():\n",
    "    ddf_bench = ddf[ddf_cols]\n",
    "else:\n",
    "    parquet_pattern = base_url + '/yellow_tripdata_2024-0*.parquet'\n",
    "    ddf_bench = dd.read_parquet(parquet_pattern, engine='pyarrow', gather_statistics=False, columns=ddf_cols)\n",
    "\n",
    "for i in range(3):\n",
    "    t0 = time.perf_counter()\n",
    "    tmp = ddf_bench.assign(pickup_date=ddf_bench.tpep_pickup_datetime.dt.date) \\\n",
    "        .groupby('pickup_date').agg({\n",
    "            'trip_distance':'mean', 'total_amount':'mean', 'passenger_count':'mean'\n",
    "        }).head(20).compute()\n",
    "    dask_times.append(time.perf_counter()-t0)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'framework':['pandas']*len(pandas_times)+['dask']*len(dask_times),\n",
    "    'seconds': pandas_times + dask_times\n",
    "})\n",
    "print('Resultados de cada repetición (s):')\n",
    "print(summary)\n",
    "print('\\nPromedios:')\n",
    "print(summary.groupby('framework')['seconds'].agg(['count','mean','std']))\n",
    "\n",
    "print('\\nObservaciones (heurísticas):')\n",
    "print('- Pandas incluye costo de materializar 300k filas + groupby en memoria.')\n",
    "print('- Dask incluye overhead de scheduler; para volúmenes pequeños puede ser más lento.')\n",
    "print('- Si aumentas el LIMIT (ej. a millones) el gap puede invertirse a favor de Dask al paralelizar.')\n",
    "\n",
    "summary.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
