{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8209c68",
   "metadata": {},
   "source": [
    "# Semana 05 - Perceptrón: Implementación desde Cero vs scikit-learn\n",
    "\n",
    "Comparación detallada entre una implementación propia (scratch) y las implementaciones de `Perceptron`, `SGDClassifier` (modo perceptrón) y un baseline `LogisticRegression` usando el dataset *Breast Cancer Wisconsin (Diagnostic)*.\n",
    "\n",
    "**Objetivos:**\n",
    "- Reforzar fundamentos matemáticos del Perceptrón.\n",
    "- Evaluar diferencias prácticas entre implementación manual y librerías.\n",
    "- Analizar hiperparámetros (learning rate, épocas, regularización).\n",
    "- Medir desempeño, robustez y tiempos.\n",
    "- Construir un flujo reproducible y extensible.\n",
    "\n",
    "> Si la descarga del dataset remoto falla (sin Internet), se usará el dataset integrado de `sklearn.datasets.load_breast_cancer` como fallback. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6bfe5e",
   "metadata": {},
   "source": [
    "## 1. Descarga y Extracción del Dataset (UCI)\n",
    "Fuente: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "ZIP directo: https://archive.ics.uci.edu/static/public/17/breast+cancer+wisconsin+diagnostic.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c074d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile, io, hashlib, time, json, math, textwrap, statistics\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "ZIP_URL = 'https://archive.ics.uci.edu/static/public/17/breast+cancer+wisconsin+diagnostic.zip'\n",
    "ZIP_PATH = DATA_DIR / 'breast_cancer_wisconsin_diagnostic.zip'\n",
    "RAW_CSV = None\n",
    "try:\n",
    "    if not ZIP_PATH.exists():\n",
    "        print('Descargando ZIP...')\n",
    "        req = Request(ZIP_URL, headers={'User-Agent':'Mozilla/5.0'})\n",
    "        with urlopen(req, timeout=30) as r: data = r.read()\n",
    "        ZIP_PATH.write_bytes(data)\n",
    "    else:\n",
    "        print('ZIP ya existe, omitiendo descarga.')\n",
    "    with zipfile.ZipFile(ZIP_PATH,'r') as z:\n",
    "        print('Contenido ZIP:', z.namelist())\n",
    "        # Buscar un .data o .csv\n",
    "        members = [m for m in z.namelist() if m.lower().endswith(('.data','.csv'))]\n",
    "        if members:\n",
    "            target = members[0]\n",
    "            csv_bytes = z.read(target)\n",
    "            # Guardar a un csv normalizado\n",
    "            RAW_CSV = DATA_DIR / 'breast_cancer_wisconsin_diagnostic.csv'\n",
    "            RAW_CSV.write_bytes(csv_bytes)\n",
    "            print('Extraído a', RAW_CSV)\n",
    "        else:\n",
    "            print('No se encontró archivo .data/.csv en ZIP, se usará fallback.')\n",
    "except Exception as e:\n",
    "    print('Fallo en descarga/lectura ZIP:', e)\n",
    "    RAW_CSV = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c003398",
   "metadata": {},
   "source": [
    "## 2. Carga Inicial y Estructura de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14804019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "if RAW_CSV and RAW_CSV.exists():\n",
    "    # El archivo original suele tener formato: ID,Diagnosis,30 features...\n",
    "    df_raw = pd.read_csv(RAW_CSV, header=None)\n",
    "    # Según documentación, las columnas: ID, diagnosis, 30 features + maybe trailing empty\n",
    "    # Para mayor robustez aplicamos shape check\n",
    "    # Usamos dataset sklearn para nombres de columnas\n",
    "    sk = load_breast_cancer()\n",
    "    feature_names = list(sk.feature_names) if hasattr(sk,'feature_names') else [f'f{i}' for i in range(30)]\n",
    "    cols = ['id','diagnosis'] + feature_names\n",
    "    if df_raw.shape[1] >= 32: # a veces incluye vacía\n",
    "        df_raw = df_raw.iloc[:, :32]\n",
    "    df_raw.columns = cols[:df_raw.shape[1]]\n",
    "else:\n",
    "    print('Usando fallback sklearn.load_breast_cancer')\n",
    "    sk = load_breast_cancer()\n",
    "    df_raw = pd.DataFrame(sk.data, columns=sk.feature_names)\n",
    "    df_raw.insert(0,'id', range(1, len(df_raw)+1))\n",
    "    df_raw.insert(1,'diagnosis', sk.target)\n",
    "    # Mapear 0/1 a B/M para mantener formato luego inverteremos\n",
    "    df_raw['diagnosis'] = df_raw['diagnosis'].map({0:'malignant',1:'benign'})\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape:', df_raw.shape)\n",
    "print('Tipos:\n",
    "', df_raw.dtypes.head())\n",
    "print('Valores diagnosis:', df_raw['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa923ecc",
   "metadata": {},
   "source": [
    "## 3. Limpieza y Conversión de Tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "# Eliminar ID si no aporta\n",
    "if 'id' in df.columns: df = df.drop(columns=['id'])\n",
    "# Mapear diagnosis a binario (1 maligno / 0 benigno). Dataset original: M=Malignant, B=Benign\n",
    "mapping = {'M':1,'B':0,'malignant':1,'benign':0}\n",
    "df['diagnosis'] = df['diagnosis'].map(mapping)\n",
    "# Asegurar tipos numéricos\n",
    "for c in df.columns:\n",
    "    if c!='diagnosis': df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "print('Nulos por columna (esperado ~0):')\n",
    "print(df.isna().sum().sort_values(ascending=False).head())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50304b1d",
   "metadata": {},
   "source": [
    "## 4. Análisis Exploratorio Básico (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe98435",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df.describe().T\n",
    "target_dist = df['diagnosis'].value_counts(normalize=True) * 100\n",
    "print(target_dist)\n",
    "corr = df.corr(numeric_only=True)\n",
    "sns.heatmap(corr.iloc[:10,:10], cmap='coolwarm', center=0); plt.title('Correlaciones (primeras 10 features)'); plt.show()\n",
    "desc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bbe83",
   "metadata": {},
   "source": [
    "## 5. Ingeniería de Características y Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40855cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_full = df.drop(columns=['diagnosis']).copy()\n",
    "y = df['diagnosis'].astype(int).values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "X_scaled[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c8061",
   "metadata": {},
   "source": [
    "## 6. División Entrenamiento / Validación / Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22230608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.30, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)\n",
    "print('Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c53370",
   "metadata": {},
   "source": [
    "## 7. Definición Matemática del Perceptrón\n",
    "\\( at{y} = \n",
    "\\begin{cases} 1 & \text{si } w^T x + b e 0 \\ 0 & \text{caso contrario} nd{cases} \\)\n",
    "Actualización: \\( w := w + ta (y-at{y}) x,  b := b + ta (y-at{y}) \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e11fca4",
   "metadata": {},
   "source": [
    "## 8. Implementación desde Cero: Clase PerceptronScratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "@dataclass\n",
    "class PerceptronScratch:\n",
    "    learning_rate: float = 0.01\n",
    "    n_epochs: int = 50\n",
    "    shuffle: bool = True\n",
    "    random_state: int = 42\n",
    "    verbose: bool = False\n",
    "    def __post_init__(self):\n",
    "        self.rng = np.random.default_rng(self.random_state)\n",
    "        self.w_ = None\n",
    "        self.b_ = 0.0\n",
    "        self.errors_ = []\n",
    "        self.acc_ = []\n",
    "    def _shuffle(self, X, y):\n",
    "        idx = self.rng.permutation(len(X))\n",
    "        return X[idx], y[idx]\n",
    "    def fit(self, X, y, X_val=None, y_val=None, early_stop=True):\n",
    "        X = np.asarray(X); y = np.asarray(y).astype(int)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w_ = self.rng.normal(0, 0.01, size=n_features)\n",
    "        self.b_ = 0.0\n",
    "        self.errors_.clear(); self.acc_.clear()\n",
    "        for epoch in range(self.n_epochs):\n",
    "            if self.shuffle: X, y = self._shuffle(X, y)\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.learning_rate * (target - self.predict_single(xi))\n",
    "                if update != 0:\n",
    "                    self.w_ += update * xi\n",
    "                    self.b_ += update\n",
    "                    errors += int(update!=0)\n",
    "            self.errors_.append(errors)\n",
    "            if X_val is not None:\n",
    "                val_pred = self.predict(X_val)\n",
    "                acc = (val_pred == y_val).mean()\n",
    "                self.acc_.append(acc)\n",
    "            if self.verbose:\n",
    "                print(f'Epoch {epoch+1}/{self.n_epochs} - errors={errors}')\n",
    "            if early_stop and errors == 0:\n",
    "                if self.verbose: print('Convergencia temprana.')\n",
    "                break\n",
    "        return self\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "    def predict_single(self, x):\n",
    "        return 1 if (np.dot(x, self.w_) + self.b_) >= 0 else 0\n",
    "    def predict(self, X):\n",
    "        return (self.net_input(X) >= 0).astype(int)\n",
    "    def decision_function(self, X):\n",
    "        return self.net_input(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c8e23",
   "metadata": {},
   "source": [
    "## 9. Función de Entrenamiento Manual (Loop de Épocas) + 10. Historial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc080d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = PerceptronScratch(learning_rate=0.001, n_epochs=100, verbose=False)\n",
    "t0=time.perf_counter()\n",
    "scratch.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n",
    "t_scratch = time.perf_counter()-t0\n",
    "print('Tiempo entrenamiento (scratch):', t_scratch)\n",
    "print('Errores por época (primeros 15):', scratch.errors_[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99360b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scratch.errors_, marker='o'); plt.title('Errores por época (Scratch)'); plt.xlabel('Época'); plt.ylabel('Errores'); plt.show()\n",
    "if scratch.acc_:\n",
    "    plt.plot(scratch.acc_); plt.title('Accuracy validación (Scratch)'); plt.xlabel('Época'); plt.ylabel('Acc'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d67f9",
   "metadata": {},
   "source": [
    "## 11. Evaluación del Modelo Manual (Métricas y Matriz de Confusión)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444867b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, classification_report\n",
    "y_pred_scratch = scratch.predict(X_test)\n",
    "metrics_scratch = {\n",
    "    'accuracy': accuracy_score(y_test,y_pred_scratch),\n",
    "    'precision': precision_score(y_test,y_pred_scratch),\n",
    "    'recall': recall_score(y_test,y_pred_scratch),\n",
    "    'f1': f1_score(y_test,y_pred_scratch)\n",
    "}\n",
    "cm_scratch = confusion_matrix(y_test,y_pred_scratch)\n",
    "metrics_scratch, cm_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222b51d",
   "metadata": {},
   "source": [
    "## 12. Visualización de Frontera (PCA 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1907848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_train_2d = pca.fit_transform(X_train)\n",
    "X_test_2d = pca.transform(X_test)\n",
    "# Ajustar un perceptrón scratch en espacio 2D solo para visualizar\n",
    "viz_model = PerceptronScratch(learning_rate=0.01, n_epochs=50).fit(X_train_2d, y_train)\n",
    "# Grid para frontera\n",
    "xx, yy = np.meshgrid(np.linspace(X_train_2d[:,0].min()-1, X_train_2d[:,0].max()+1, 200), np.linspace(X_train_2d[:,1].min()-1, X_train_2d[:,1].max()+1, 200))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "zz = viz_model.predict(grid).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, alpha=0.25, cmap='coolwarm');\n",
    "sns.scatterplot(x=X_train_2d[:,0], y=X_train_2d[:,1], hue=y_train, palette='coolwarm', s=30, edgecolor='k');\n",
    "plt.title('Frontera (PCA 2D)'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d62bb",
   "metadata": {},
   "source": [
    "## 13. Ajuste de Hiperparámetros (lr, épocas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lr=[0.0005,0.001,0.01,0.1]\n",
    "grid_epochs=[25,50,100]\n",
    "results=[]\n",
    "for lr in grid_lr:\n",
    "    for ne in grid_epochs:\n",
    "        m=PerceptronScratch(learning_rate=lr,n_epochs=ne)\n",
    "        m.fit(X_train,y_train,X_val=X_val,y_val=y_val,early_stop=True)\n",
    "        yv=m.predict(X_val)\n",
    "        results.append({'lr':lr,'epochs':ne,'val_acc':(yv==y_val).mean(),'final_errors':m.errors_[-1]})\n",
    "hp_df = pd.DataFrame(results).sort_values('val_acc', ascending=False)\n",
    "hp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9ba93",
   "metadata": {},
   "source": [
    "## 14. Implementación scikit-learn (Perceptron, SGDClassifier, LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b346e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron, SGDClassifier, LogisticRegression\n",
    "t0=time.perf_counter(); sk_perc = Perceptron(max_iter=1000, eta0=1.0, random_state=42, tol=1e-3); sk_perc.fit(X_train,y_train); t_perc=time.perf_counter()-t0\n",
    "t0=time.perf_counter(); sk_sgd = SGDClassifier(loss='perceptron', learning_rate='constant', eta0=0.01, max_iter=1000, random_state=42, tol=1e-3); sk_sgd.fit(X_train,y_train); t_sgd=time.perf_counter()-t0\n",
    "t0=time.perf_counter(); sk_log = LogisticRegression(max_iter=1000, random_state=42); sk_log.fit(X_train,y_train); t_log=time.perf_counter()-t0\n",
    "models = {'scratch': (scratch, t_scratch), 'sk_perceptron': (sk_perc, t_perc), 'sk_sgd_perc': (sk_sgd, t_sgd), 'log_reg': (sk_log, t_log)}\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a0ca3",
   "metadata": {},
   "source": [
    "## 15. Comparación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f75bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[]\n",
    "for name,(model,t) in models.items():\n",
    "    ypred = model.predict(X_test)\n",
    "    rows.append({'modelo':name,'accuracy':accuracy_score(y_test,ypred),'precision':precision_score(y_test,ypred),'recall':recall_score(y_test,ypred),'f1':f1_score(y_test,ypred),'tiempo_s':t})\n",
    "cmp_df = pd.DataFrame(rows).sort_values('f1', ascending=False)\n",
    "cmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabed67a",
   "metadata": {},
   "source": [
    "## 16. Curvas ROC y AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for name,(model,_) in models.items():\n",
    "    if hasattr(model,'decision_function'):\n",
    "        scores = model.decision_function(X_test)\n",
    "    else:\n",
    "        # probas para logistic\n",
    "        scores = model.predict_proba(X_test)[:,1]\n",
    "    fpr,tpr,_ = roc_curve(y_test, scores)\n",
    "    auc_val = auc(fpr,tpr)\n",
    "    plt.plot(fpr,tpr,label=f\n",
    ")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curves'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687af2ed",
   "metadata": {},
   "source": [
    "## 17. Validación Cruzada Estratificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2faa9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_score_scratch(lr=0.001, epochs=50, k=5):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores=[]\n",
    "    for tr, va in skf.split(X_train, y_train):\n",
    "        m=PerceptronScratch(learning_rate=lr,n_epochs=epochs)\n",
    "        m.fit(X_train[tr], y_train[tr])\n",
    "        pred = m.predict(X_train[va])\n",
    "        scores.append((pred==y_train[va]).mean())\n",
    "    return np.mean(scores), np.std(scores)\n",
    "mean_cv, std_cv = cv_score_scratch()\n",
    "print('Scratch CV acc mean±std:', f\n",
    ")\n",
    "# sklearn Perceptron CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores=[]\n",
    "for tr,va in skf.split(X_train,y_train):\n",
    "    m=Perceptron(max_iter=1000, random_state=42)\n",
    "    m.fit(X_train[tr],y_train[tr])\n",
    "    scores.append(m.score(X_train[va],y_train[va]))\n",
    "print('sklearn Perceptron CV acc:', f\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3949099",
   "metadata": {},
   "source": [
    "## 18. Manejo de Desbalance de Clases (class_weight / re-muestreo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dist = pd.Series(y).value_counts(normalize=True)\n",
    "print('Distribución clases (%):', class_dist*100)\n",
    "# Ejemplo de class_weight con sklearn Perceptron\n",
    "cw_model = Perceptron(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "cw_model.fit(X_train,y_train)\n",
    "print('Balanced accuracy test:', cw_model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd958191",
   "metadata": {},
   "source": [
    "## 19. Medición de Tiempos y Complejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_train(model, Xtr, ytr):\n",
    "    t0=time.perf_counter(); model.fit(Xtr,ytr); return time.perf_counter()-t0\n",
    "times = {name: t for name,(_,t) in models.items()}\n",
    "pd.Series(times).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64959538",
   "metadata": {},
   "source": [
    "## 20. Persistencia de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Guardar pesos scratch\n",
    "scratch_artifact = {'w': scratch.w_.tolist(), 'b': scratch.b_, 'learning_rate': scratch.learning_rate, 'epochs': len(scratch.errors_)}\n",
    "with open('perceptron_scratch.json','w') as f: json.dump(scratch_artifact,f)\n",
    "joblib.dump(sk_perc,'perceptron_sklearn.joblib')\n",
    "print('Artefactos guardados.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c050b",
   "metadata": {},
   "source": [
    "## 21. Pruebas Unitarias (concepto)\n",
    "Ejemplos de aserciones rápidas; ideal mover a archivo test separado con pytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: dimensiones pesos\n",
    "assert scratch.w_.shape[0] == X_train.shape[1]\n",
    "# Test: predict vs decision sign consistency\n",
    "test_scores = scratch.decision_function(X_test[:5])\n",
    "assert np.all((test_scores>=0).astype(int) == scratch.predict(X_test[:5]))\n",
    "print('Pruebas básicas OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be50b59",
   "metadata": {},
   "source": [
    "## 22. Pipeline + GridSearchCV (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe = Pipeline([('clf', Perceptron(random_state=42))])\n",
    "param_grid = {'clf__penalty':[None,'l2','l1'],'clf__alpha':[0.0001,0.001,0.01]}\n",
    "gs = GridSearchCV(pipe, param_grid, scoring='f1', cv=5, n_jobs=-1)\n",
    "gs.fit(X_train,y_train)\n",
    "print('Best params:', gs.best_params_, 'Best f1:', gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d3206",
   "metadata": {},
   "source": [
    "## 23. Resumen Programático de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sklearn = gs.best_estimator_\n",
    "y_best = best_sklearn.predict(X_test)\n",
    "summary_rows = []\n",
    "for name,(model,t) in models.items():\n",
    "    pred = model.predict(X_test)\n",
    "    summary_rows.append({'modelo':name,'accuracy':accuracy_score(y_test,pred),'f1':f1_score(y_test,pred),'tiempo_s':t})\n",
    "summary_rows.append({'modelo':'best_gridsearch','accuracy':accuracy_score(y_test,y_best),'f1':f1_score(y_test,y_best),'tiempo_s':None})\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values('f1', ascending=False)\n",
    "summary_df.to_csv('resumen_modelos.csv', index=False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98898d",
   "metadata": {},
   "source": [
    "## Conclusiones y Trabajo Futuro\n",
    "- El perceptrón scratch ofrece transparencia pero menor optimización.\n",
    "- scikit-learn maneja mejor early stopping, regularización y eficiencia.\n",
    "- Logistic Regression y variantes con regularización suelen superar en estabilidad al perceptrón puro.\n",
    "- Próximos pasos: ampliar a validación estratificada repetida, calibración de probabilidades, incorporación de regularización L1/L2 en versión scratch, y comparación con SVM lineal.\n",
    "\n",
    "---\n",
    "Notebook generado para propósito educativo (Semana 05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) Mover artefactos generados a carpeta artifacts/\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "art_dir = Path('artifacts')\n",
    "art_dir.mkdir(exist_ok=True)\n",
    "for fname in ['perceptron_scratch.json','perceptron_sklearn.joblib','resumen_modelos.csv']:\n",
    "    f = Path(fname)\n",
    "    if f.exists():\n",
    "        target = art_dir / f.name\n",
    "        shutil.move(str(f), target)\n",
    "        print(f'Movido {f} -> {target}')\n",
    "print('Resumen contenido artifacts:', list(art_dir.iterdir()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
